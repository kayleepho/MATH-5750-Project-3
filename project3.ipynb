{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kayleepho/MATH-5750-Project-3/blob/main/project3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Math 5750/6880: Mathematics of Data Science \\\n",
        "Project 3"
      ],
      "metadata": {
        "id": "0gdC70xxFyc4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Fashion-MNIST image classification using sklearn\n",
        "In this exercise, you will build a\n",
        "classifier for the Fashion-MNIST image dataset using the sklearn MLPClassifier.\\\\\n",
        "- Use the provided code to import and preprocess the Fashion-MNIST image dataset. \\\\\n",
        "- Build a dense neural network\n",
        "using MLPClassifier. \\\\\n",
        "- Start with a simple model architecture and train your model.\\\\\n",
        "- Then experiment with your model/training method by changing:\\\\\n",
        "• the number of hidden layers and neurons\\\\\n",
        "• the activation functions\\\\\n",
        "• optimization method and the learning rate\\\\\n",
        "• regularization or early stopping settings.\\\\\n",
        "- Observe how each change affects convergence speed and accuracy."
      ],
      "metadata": {
        "id": "i9_7SnpMGKDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Fashion-MNIST\n",
        "# Classes (0-9): T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "X_train = X_train.reshape(len(X_train), -1)\n",
        "X_test  = X_test.reshape(len(X_test), -1)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "AB136H0PGKq1",
        "outputId": "52441ce7-0bfd-4ff2-a43d-4d691298e61d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Build a dense neural network using MLPClassifier.\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = mlp.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "5GAsN-dmHjRM",
        "outputId": "3fdc144c-3853-4ae1-d7c7-a9743658dcb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8836\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing the number of hidden layers and neurons\n",
        "mlp1 = MLPClassifier(hidden_layer_sizes=(100, 50), activation='relu', solver='adam', random_state=42)\n",
        "mlp1.fit(X_train, y_train)\n",
        "y_pred1 = mlp1.predict(X_test)\n",
        "accuracy1 = accuracy_score(y_test, y_pred1)\n",
        "print(f\"Accuracy: {accuracy1}\")"
      ],
      "metadata": {
        "id": "sWGekiZ3FDvu",
        "outputId": "aea8e339-ba21-4601-b402-c09fd28bf5a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8802\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing the activation functions\n",
        "mlp2 = MLPClassifier(hidden_layer_sizes=(100,), activation='tanh', solver='adam', random_state=42)\n",
        "mlp2.fit(X_train, y_train)\n",
        "y_pred2 = mlp2.predict(X_test)\n",
        "accuracy2 = accuracy_score(y_test, y_pred2)\n",
        "print(f\"Accuracy: {accuracy2}\")"
      ],
      "metadata": {
        "id": "2F0az6bKFWaH",
        "outputId": "d7d2f502-7fee-4ff6-dca2-24e8685c617c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8741\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing optimization method and the learning rate\n",
        "mlp3 = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='sgd', learning_rate_init=0.01, random_state=42)\n",
        "mlp3.fit(X_train, y_train)\n",
        "y_pred3 = mlp3.predict(X_test)\n",
        "accuracy3 = accuracy_score(y_test, y_pred3)\n",
        "print(f\"Accuracy: {accuracy3}\")"
      ],
      "metadata": {
        "id": "LXLQTbU6FdPm",
        "outputId": "35ffa387-df51-4979-aa2b-894c60caae55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8801\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing regularization or early stopping settings\n",
        "mlp4 = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam', early_stopping=True, random_state=42)\n",
        "mlp4.fit(X_train, y_train)\n",
        "y_pred4 = mlp4.predict(X_test)\n",
        "accuracy4 = accuracy_score(y_test, y_pred4)\n",
        "print(f\"Accuracy: {accuracy4}\")"
      ],
      "metadata": {
        "id": "i_eKeIblFmrf",
        "outputId": "9384b3ee-17d8-448e-c3dd-0829bb8d08a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8832\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# ----------------------------\n",
        "# 1. Load and preprocess data\n",
        "# ----------------------------\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Flatten images: 28x28 -> 784\n",
        "X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test  = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "# Check shapes\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test  = scaler.transform(X_test)\n",
        "\n",
        "# ----------------------------\n",
        "# 2. Define model configurations\n",
        "# ----------------------------\n",
        "configs = [\n",
        "    (\"Baseline\", dict(hidden_layer_sizes=(100,), activation='relu', solver='adam', random_state=42)),\n",
        "    (\"Deeper network\", dict(hidden_layer_sizes=(100, 50), activation='relu', solver='adam', random_state=42)),\n",
        "    (\"Tanh activation\", dict(hidden_layer_sizes=(100,), activation='tanh', solver='adam', random_state=42)),\n",
        "    (\"SGD optimizer\", dict(hidden_layer_sizes=(100,), activation='relu', solver='sgd', learning_rate_init=0.01, random_state=42)),\n",
        "    (\"Regularized + EarlyStop\", dict(hidden_layer_sizes=(100,), activation='relu', solver='adam', early_stopping=True, random_state=42))\n",
        "]\n",
        "\n",
        "# ----------------------------\n",
        "# 3. Train models and record results\n",
        "# ----------------------------\n",
        "results = []\n",
        "\n",
        "for name, params in configs:\n",
        "    print(f\"\\nTraining model: {name}\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    mlp = MLPClassifier(**params)\n",
        "    mlp.fit(X_train, y_train)  # Should now work without 4D error\n",
        "\n",
        "    train_time = time.time() - start_time\n",
        "    accuracy = accuracy_score(y_test, mlp.predict(X_test))\n",
        "\n",
        "    results.append({\n",
        "        \"Model\": name,\n",
        "        \"Accuracy\": accuracy,\n",
        "        \"Training Time (s)\": train_time,\n",
        "        \"Iterations\": mlp.n_iter_\n",
        "    })\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}, Training Time: {train_time:.2f}s, Iterations: {mlp.n_iter_}\")\n",
        "\n",
        "# ----------------------------\n",
        "# 4. Display results\n",
        "# ----------------------------\n",
        "df_results = pd.DataFrame(results)\n",
        "print(\"\\nSummary of experiments:\")\n",
        "print(df_results)\n"
      ],
      "metadata": {
        "id": "CjPhkfPKFrpe",
        "outputId": "cb3c7633-5707-4e48-9107-244e521192fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (60000, 784)\n",
            "X_test shape: (10000, 784)\n",
            "\n",
            "Training model: Baseline\n",
            "Accuracy: 0.8836, Training Time: 268.70s, Iterations: 124\n",
            "\n",
            "Training model: Deeper network\n",
            "Accuracy: 0.8802, Training Time: 208.99s, Iterations: 93\n",
            "\n",
            "Training model: Tanh activation\n",
            "Accuracy: 0.8741, Training Time: 243.50s, Iterations: 104\n",
            "\n",
            "Training model: SGD optimizer\n",
            "Accuracy: 0.8801, Training Time: 319.21s, Iterations: 184\n",
            "\n",
            "Training model: Regularized + EarlyStop\n",
            "Accuracy: 0.8832, Training Time: 53.07s, Iterations: 30\n",
            "\n",
            "Summary of experiments:\n",
            "                     Model  Accuracy  Training Time (s)  Iterations\n",
            "0                 Baseline    0.8836         268.700726         124\n",
            "1           Deeper network    0.8802         208.994771          93\n",
            "2          Tanh activation    0.8741         243.498878         104\n",
            "3            SGD optimizer    0.8801         319.212848         184\n",
            "4  Regularized + EarlyStop    0.8832          53.067917          30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Fashion-MNIST image classification  using pytorch\n",
        "In this exercise, you will repeat Exercise 1 using PyTorch.\n",
        "- Use the provided code to import and preprocess the Fashion-MNIST image dataset.\n",
        "- Again, start with a simple model architecture and train your model.\n",
        "- As above, experiment with the model/training method.\n",
        "- With pytorch, there are many more options, so read about them and experiment!\n",
        "- Try to further improve your model by using convolutional neural network (CNN) layers and MaxPool2d layers.\n",
        "- For a challenge, use transfer learning to import a pretrained model and fine\n",
        "tune it on the Fashion-MNIST image dataset."
      ],
      "metadata": {
        "id": "a2qcKggmIH8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Load Fashion-MNIST\n",
        "# Classes (0-9): T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# scale to [0,1], add channel dimension -> (N, 1, 28, 28)\n",
        "X_train = (X_train.astype(\"float32\") / 255.0)[:, None, :, :]\n",
        "X_test  = (X_test.astype(\"float32\")  / 255.0)[:,  None, :, :]\n",
        "\n",
        "y_train = y_train.astype(np.int64)\n",
        "y_test  = y_test.astype(np.int64)\n",
        "\n",
        "# train/val split: last 10k of train as validation\n",
        "X_tr, X_val = X_train[:50000], X_train[50000:]\n",
        "y_tr, y_val = y_train[:50000], y_train[50000:]\n",
        "\n",
        "# wrap in PyTorch TensorDatasets and DataLoaders\n",
        "train_ds = TensorDataset(torch.from_numpy(X_tr),  torch.from_numpy(y_tr))\n",
        "val_ds   = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
        "test_ds  = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=256, shuffle=False)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=256, shuffle=False)"
      ],
      "metadata": {
        "id": "B9IQwhgcIVOl",
        "outputId": "bdc68657-f3c6-4243-a996-a06621f8d1c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# In colab, you should ``change runtime type'' to GPU.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ],
      "metadata": {
        "id": "0REsDBunNmEl",
        "outputId": "aa23adb5-d088-410e-8ace-19557fd1aca4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start with a simple model architecture\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, activation='relu'):\n",
        "        super().__init__()\n",
        "        act_fn = nn.ReLU() if activation == 'relu' else nn.Tanh()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
        "            act_fn,\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            act_fn,\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64 * 7 * 7, 128),\n",
        "            act_fn,\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "Aq5ksc_r3wj_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train your model\n",
        "def train_model(model, optimizer, train_loader, val_loader, n_epochs=5):\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    history = []\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(xb)\n",
        "            loss = criterion(pred, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Validation accuracy\n",
        "        model.eval()\n",
        "        correct, total = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                pred = model(xb)\n",
        "                correct += (pred.argmax(1) == yb).sum().item()\n",
        "                total += yb.size(0)\n",
        "\n",
        "        val_acc = correct / total\n",
        "        print(f\"Epoch {epoch+1}: loss={total_loss/len(train_loader):.4f}, val_acc={val_acc:.4f}\")\n",
        "        history.append(val_acc)\n",
        "    return history"
      ],
      "metadata": {
        "id": "10y-BkPZ4zKl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_accuracy(model, test_loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in test_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            pred = model(xb)\n",
        "            correct += (pred.argmax(1) == yb).sum().item()\n",
        "            total += yb.size(0)\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "PqMHHotO5CJp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment with the model/training method\n",
        "configs = [\n",
        "    (\"Baseline (ReLU + Adam)\", SimpleCNN('relu'), optim.Adam),\n",
        "    (\"Tanh activation\", SimpleCNN('tanh'), optim.Adam),\n",
        "    (\"SGD optimizer\", SimpleCNN('relu'), optim.SGD),\n",
        "]\n",
        "\n",
        "results = []\n",
        "\n",
        "for name, model, opt_class in configs:\n",
        "    print(f\"\\nTraining: {name}\")\n",
        "    optimizer = opt_class(model.parameters(), lr=0.001 if opt_class == optim.Adam else 0.01)\n",
        "    train_model(model, optimizer, train_loader, val_loader, n_epochs=5)\n",
        "    acc = test_accuracy(model, test_loader)\n",
        "    print(f\"Test accuracy for {name}: {acc:.4f}\")\n",
        "    results.append((name, acc))"
      ],
      "metadata": {
        "id": "fxOLI4Hf5TG_",
        "outputId": "32de76c3-25ed-458f-fb4d-2039f45a6c45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training: Baseline (ReLU + Adam)\n",
            "Epoch 1: loss=0.5407, val_acc=0.8695\n",
            "Epoch 2: loss=0.3351, val_acc=0.8873\n",
            "Epoch 3: loss=0.2846, val_acc=0.8896\n",
            "Epoch 4: loss=0.2547, val_acc=0.9025\n",
            "Epoch 5: loss=0.2315, val_acc=0.9067\n",
            "Test accuracy for Baseline (ReLU + Adam): 0.9009\n",
            "\n",
            "Training: Tanh activation\n",
            "Epoch 1: loss=0.4831, val_acc=0.8719\n",
            "Epoch 2: loss=0.3116, val_acc=0.8888\n",
            "Epoch 3: loss=0.2688, val_acc=0.8993\n",
            "Epoch 4: loss=0.2357, val_acc=0.9064\n",
            "Epoch 5: loss=0.2122, val_acc=0.8959\n",
            "Test accuracy for Tanh activation: 0.8905\n",
            "\n",
            "Training: SGD optimizer\n",
            "Epoch 1: loss=1.4808, val_acc=0.7240\n",
            "Epoch 2: loss=0.7339, val_acc=0.7360\n",
            "Epoch 3: loss=0.6439, val_acc=0.7606\n",
            "Epoch 4: loss=0.5932, val_acc=0.7800\n",
            "Epoch 5: loss=0.5526, val_acc=0.7647\n",
            "Test accuracy for SGD optimizer: 0.7563\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_results = pd.DataFrame(results, columns=[\"Model\", \"Test Accuracy\"])\n",
        "print(\"\\nSummary of experiments:\")\n",
        "print(df_results)"
      ],
      "metadata": {
        "id": "5i_r08S95pYp",
        "outputId": "d8ecc93c-af76-4e06-90e7-58e6765afddf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary of experiments:\n",
            "                    Model  Test Accuracy\n",
            "0  Baseline (ReLU + Adam)         0.9009\n",
            "1         Tanh activation         0.8905\n",
            "2           SGD optimizer         0.7563\n"
          ]
        }
      ]
    }
  ]
}